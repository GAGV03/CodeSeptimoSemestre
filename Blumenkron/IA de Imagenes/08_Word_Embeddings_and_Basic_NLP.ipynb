{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOzf4YxD3pDYjRgSgbjrtDO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0ZV57w1AeZcU"},"source":["# Word Embeddings for Natural Language Processing\n","In this notebook we'll look at some simple examples of working with text using word embeddings and recurrent neural networks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpN7m2-6HHqs"},"outputs":[],"source":["import io\n","import re\n","import string\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"Q6mJg1g3apaz"},"source":["## Representing text as numbers\n","\n","Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, you will look at three strategies for doing so.\n","\n","### One-hot encodings\n","\n","As a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n","\n","<img src=\"https://github.com/tensorflow/text/blob/master/docs/guide/images/one-hot.png?raw=1\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n","\n","To create a vector that contains the encoding of the sentence, you could then concatenate the one-hot vectors for each word.\n","\n","Key point: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n","\n","### Encode each word with a unique number\n","\n","A second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This approach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n","\n","There are two downsides to this approach, however:\n","\n","* The integer-encoding is arbitrary (it does not capture any relationship between words).\n","\n","* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n","\n","### Word embeddings\n","\n","Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n","\n","<img src=\"https://github.com/tensorflow/text/blob/master/docs/guide/images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n","\n","Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table."]},{"cell_type":"markdown","metadata":{"id":"SBFctV8-JZOc"},"source":["## Download the IMDb Dataset\n","You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. The IMDb dataset consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the famous Internet Movie Database, along with a simple binary target for each review indicating whether it is negative (0) or positive (1). You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch.\n","\n","Download the dataset using Keras file utility and take a look at the directories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GKh_pFTeZcn"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","\n","raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n","    name=\"imdb_reviews\",\n","    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n","    as_supervised=True\n",")\n","tf.random.set_seed(42)\n","train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n","valid_set = raw_valid_set.batch(32).prefetch(1)\n","test_set = raw_test_set.batch(32).prefetch(1)"]},{"cell_type":"markdown","metadata":{"id":"eHa6cq0-Ym0g"},"source":["Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJ3eOpDOeZcn"},"outputs":[],"source":["for review, label in raw_train_set.take(4):\n","    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n","    print(\"Label:\", label.numpy())"]},{"cell_type":"markdown","metadata":{"id":"eqBazMiVQkj1"},"source":["## Using the Embedding layer\n","\n","Keras makes it easy to use word embeddings. Take a look at the [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n","\n","The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OjxLVrMvWUE"},"outputs":[],"source":["# Embed a 1,000 word vocabulary into 5 dimensions.\n","embedding_layer = tf.keras.layers.Embedding(1000, 5)"]},{"cell_type":"markdown","metadata":{"id":"2dKKV1L2Rk7e"},"source":["When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n","\n","If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YUjPgP7w0PO"},"outputs":[],"source":["result = embedding_layer(tf.constant([1, 2, 3]))\n","result.numpy()"]},{"cell_type":"markdown","metadata":{"id":"O4PC4QzsxTGx"},"source":["For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n","\n","The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwSYepRjyRGy"},"outputs":[],"source":["result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n","result.shape"]},{"cell_type":"markdown","metadata":{"id":"WGQp2N92yOyB"},"source":["When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses an RNN architecture."]},{"cell_type":"markdown","metadata":{"id":"aGicgV5qT0wh"},"source":["## Text preprocessing"]},{"cell_type":"markdown","metadata":{"id":"N6NZSqIIoU0Y"},"source":["Next, to build a model for this task, we need to preprocess the text, first we will chop it into words, for this, we can use the [`tf.keras.layers.TextVectorization`](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/) layer. Note that it uses spaces to identify word boundaries.\n","\n","We will limit the vocabulary to 10,000 tokens, including the most frequent 9998 words plus a padding token and a token for unknown words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MlsXzo-ZlfK"},"outputs":[],"source":["# Create a custom standardization function to strip HTML break tags '<br />'.\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n","  return tf.strings.regex_replace(stripped_html,\n","                                  '[%s]' % re.escape(string.punctuation), '')\n","\n","\n","# Vocabulary size and number of words in a sequence.\n","vocab_size = 10000\n","sequence_length = 100\n","\n","# Use the text vectorization layer to normalize, split, and map strings to\n","# integers. Note that the layer uses the custom standardization defined above.\n","# Set maximum_sequence length as all samples are not of the same length.\n","vectorize_layer = tf.keras.layers.TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)\n","\n","# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n","text_ds = train_set.map(lambda x, y: x)\n","vectorize_layer.adapt(text_ds)"]},{"cell_type":"markdown","metadata":{"id":"FUs1YBcqeZcn"},"source":["Finally, we can create the model and train it.\n","\n","The first layer is the `TextVectorization` layer we just prepared, followed by an [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer that will convert word IDs into embeddings. The embedding matrix needs to have one row per token in the vocabulary (`vocab_size`) and one column per embedding dimension (this example uses 128 dimensions, but this is a hyperparameter you could tune). Next we use a `GRU` layer and a `Dense` layer with a single neuron and the sigmoid activation function, since this is a binary classification task: the model's output will be the estimated probability that the review expresses a positive sentiment regarding the movie.\n","\n","One final element to consider is that the reviews have different lengths, so when the `TextVectorization` layer converts them to sequences of token IDs, it'll pad the shorter sequences using the padding token (with ID 0) to make them as long as the longest sequence in the batch. As a result, most sequences will end with many padding tokens (often dozens or even hundreds of them). Even though we're using a `GRU` layer, which is much better than a `SimpleRNN` layer, its short-term memory is still not great, so when it goes through many padding tokens, it will inevitably end up forgetting what the review was about! One solution is to feed the model with batches of equal-length sentences (which also speeds up training). Another solution is to make the RNN ignore the padding tokens. This can be done using masking. Making the model ignore padding tokens is trivial using Keras: simply add `mask_zero=True` when creating the `Embedding` layer. This means that padding tokens (whose ID is 0) will be ignored by all downstream layers. That's it!\n","\n","We then compile the model, and we fit it on the dataset we prepared earlier for a couple of epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcj8aFsYeZco"},"outputs":[],"source":["embed_size = 128\n","tf.random.set_seed(42)\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n","                                                  patience=3,\n","                                                  restore_best_weights=True)\n","\n","model = tf.keras.Sequential([\n","    vectorize_layer,\n","    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True,\n","                              name=\"embedding\"),\n","    tf.keras.layers.GRU(64),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(loss=\"binary_crossentropy\",\n","              optimizer=\"nadam\", metrics=[\"accuracy\"])\n","\n","model.summary()\n","\n","history = model.fit(train_set, validation_data=valid_set,\n","                    epochs=10, callbacks=[early_stopping])"]},{"cell_type":"markdown","source":["We can now use our model to predict the sentiment of some new reviews."],"metadata":{"id":"c-pLdjk3V7oB"}},{"cell_type":"code","source":["reviews = np.array([\"The film is, for the most part, a litany of bad, embarrassing or lowest-common-denominator choices, and it’s unsurprising that the love interest from previous films, Michelle Williams, has opted to cut and run with this new one.\",\n","                    \"Why anyone thought this would work as a musical is absolutely baffling, especially when your male lead actor can’t carry a tune.\",\n","                    \"The practical effects on display are a masterclass of grotesque horror, and Art the Clown is everything viewers will crave from this murderous demon.\",\n","                    \"It is bigger and ultimately better than the first because it doubles down on this outlandish but creative premise and delivers some surprisingly shocking scares.\",\n","                    \"This film not only delivers laughs but also proves that even after all these years, Burton’s magic is still enchanting.\"])\n","y_pred = model.predict(reviews.astype('object'))\n","y_pred = np.where(y_pred > 0.5, 1, 0)\n","print(y_pred)"],"metadata":{"id":"OxMX3BtFONta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCoA6qwqP836"},"source":["## Retrieve the trained word embeddings and save them to disk\n","\n","Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`."]},{"cell_type":"markdown","metadata":{"id":"Zp5rv01WG2YA"},"source":["Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Uamp1YH8RzU"},"outputs":[],"source":["weights = model.get_layer('embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()"]},{"cell_type":"markdown","metadata":{"id":"J8MiCA77X8B8"},"source":["Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLIahl9s53XT"},"outputs":[],"source":["out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n","out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()"]},{"cell_type":"markdown","metadata":{"id":"JQyMZWyxYjMr"},"source":["In Google Colaboratory, you can use the following snippet to download these files to your local machine."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUsjQOKMIV2z"},"outputs":[],"source":["try:\n","  from google.colab import files\n","  files.download('vectors.tsv')\n","  files.download('metadata.tsv')\n","except Exception:\n","  pass"]},{"cell_type":"markdown","metadata":{"id":"PXLfFA54Yz-o"},"source":["## Visualize the embeddings\n","\n","To visualize the embeddings, upload them to the embedding projector.\n","\n","Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n","\n","* Click on \"Load data\".\n","\n","* Upload the two files you created above: `vecs.tsv` and `meta.tsv`.\n","\n","The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\".\n","\n","Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.\n"]},{"cell_type":"markdown","source":["This tutorial has been adapted from [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) by Aurélien Géron and the [Word Embeddings Tutorial](https://www.tensorflow.org/text/guide/word_embeddings) by the Tensorflow Authors."],"metadata":{"id":"sAduR86fw6wm"}}]}