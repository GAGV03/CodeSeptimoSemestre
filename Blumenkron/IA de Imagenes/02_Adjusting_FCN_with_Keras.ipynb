{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OS8sRpp6qEID"},"source":["# Adjusting FCN with Keras\n","\n","In the previous tutorial we covered the basic workflow for building, compiling, training and testing NN with Keras, however each of these steps can be customized to tweak the model and achieve better results. In this tutorial, we'll introduce some of these adjustable features before applying them to an image classification task."]},{"cell_type":"markdown","metadata":{"id":"QViyqZSZnuXZ"},"source":["## Weight Initialization\n","\n","By default, Keras uses Glorot initialization with a uniform distribution to assign the initial layer's weights. When creating a layer, you can change this behaviour via the `kernel_initializer` parameter to either `\"he_uniform\"`, `\"he_normal\"` or `\"lecun_normal\"`.\n","\n","Additionally, you can define your own initialization scheme through the `VarianceScaling` initializer. For example, we might define a He initialization with a uniform distribution, but based on $fan_{avg}$ instead of $fan_{in}$ like this:\n","\n","```python\n","he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n","                                                 distribution='uniform')\n","keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n","```\n","\n","For more information check the documentation: https://keras.io/api/layers/initializers/"]},{"cell_type":"markdown","metadata":{"id":"nuLBZOQ_sV2D"},"source":["## Activation Functions ##\n","\n","Activation functions can be directly specified by the `activation` argument when creating a layer (if omitted no activation function is used) or added directly to the model as an `ActivationLayer`. Built-in activation functions can be simply passed through their string identifier (for instance `'relu'` for `keras.activations.relu`).\n","\n","The following 3 examples all add a dense 10-neuron layer with a ReLU activation function:\n","\n","```python\n","model.add(keras.layers.Dense(10, activation=\"relu\"))\n","```\n","```python\n","model.add(keras.layers.Dense(10, activation=keras.activations.relu))\n","```\n","```python\n","model.add(keras.layers.Dense(10))\n","model.add(keras.layers.Activation(keras.activations.relu))\n","```\n","\n","Additionally, you can customize existing activation functions through their respective initializers.\n","\n","You can check all available activation functions in the documentation: https://keras.io/api/layers/activations/\n"]},{"cell_type":"markdown","metadata":{"id":"Cb036zQ9yHyK"},"source":["## Optimizers\n","\n","An optimizer is one of the two arguments required for compiling a Keras model (the other being a Loss function). You can either instantiate an optimizer before passing it to `model.compile()`, or you can pass it by its string identifier. For example, the following two code snippets have the same result:\n","\n","```python\n","opt = keras.optimizers.SGD()\n","model.compile(loss='categorical_crossentropy', optimizer=opt)\n","```\n","```python\n","model.compile(loss='categorical_crossentropy', optimizer='sgd')\n","```\n","\n","However, instantiating the optimizer allows us to specify different parameters than the default ones. For instance, we can specify the use of momentum on a simple SGD optimizer via the `momentum` parameter.\n","\n","```python\n","keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n","```\n","\n","We can also use Nesterov momentum by simply setting the `nesterov` parameter to `True`.\n","\n","```python\n","keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n","```\n","\n","Further, we can set a learning rate schedule to establish how the learning rate of our optimizer changes over time.\n","\n","```python\n","lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=10000,\n","    decay_rate=0.9)\n","optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n","```\n","\n","We can also choose a different optimizer by using the corresponding class.\n","\n","```python\n","# RMSProp\n","keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n","\n","# Adam\n","keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n","```\n","\n","A full list of available optimizers can be found in the documentation: https://keras.io/api/optimizers/"]},{"cell_type":"markdown","metadata":{"id":"GhwZuKXWso-Y"},"source":["## Batch Normalization ##\n","\n","A batch normalization layer normalizes its inputs. Typically used so the output of a layer is normalized before feeding it into the next layer. Batch normalization can be easily used in Keras by adding a `BatchNormalization` layer anywhere in your model. Normally, we use BN layers before or after a hidden layer's activation function and/or as the first layer of the model.\n","\n","```python\n","model.add(keras.layers.BatchNormalization())\n","```\n","\n","BN layers will behave slightly different during training and inference (predicting). During training, the layer normalizes its output using the mean and standard deviation of the current batch of inputs. During inference, the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training.\n","\n","You can read more about BN in the documentation: https://keras.io/api/layers/normalization_layers/batch_normalization/"]},{"cell_type":"markdown","metadata":{"id":"bfeieG6cu9YN"},"source":["## Gradient Clipping\n","\n","To implement gradient clipping in Keras we only need to set the `clipvalue` argument when creating an optimizer. For example, the following code will clip every component of the gradient vector to a value between -1.0 and 1.0.\n","\n","```python\n","keras.optimizer.SGD(clipvalue=1.0)\n","```"]},{"cell_type":"markdown","metadata":{"id":"v1dWzxG60CQU"},"source":["## Regularization\n","\n","Regularization terms can be added to a layer through the `kernel_regularizer` parameter to either `keras.regularizers.l1()`, `keras.regularizers.l2()` or `keras.regularizers.l1_l2()`.\n","\n","```python\n","keras.layers.Dense(100,activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n","```"]},{"cell_type":"markdown","metadata":{"id":"k8t50cl50xpH"},"source":["## Dropout\n","\n","A Dropout layer randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by $1/(1 - rate)$ such that the sum over all inputs is unchanged.\n","\n","To implement droput in Keras we simply add a `Dropout` layer before each layer.\n","\n","```python\n","model.add(keras.layers.Dropout(rate=0.03))\n","```\n","\n","Like `BatchNormalization`, `Dropout` layers exhibit a different behaviour during training than inference. In short, `Dropout` layers will only block out inputs during training, and will be disablaed during inference.\n","\n","For more on Dropout layers check the documentation: https://keras.io/api/layers/regularization_layers/dropout/"]},{"cell_type":"markdown","metadata":{"id":"PwMkk7jxdnvA"},"source":["## Callbacks ##\n","\n","The `fit()` method accepts a `callbacks` argument that lets us specify a list of objects that Keras will call at different intervals (before/after training, each epoch or each batch).\n","\n","`EarlyStopping`, for instance, will automatically interrupt training when it measures no progress on the validation set for a certain number of epochs (defined via the `patience` parameter), and it will optionally roll back to the best model at the end (if `restore_best_weights` is set to `True`).\n","\n","```python\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[early_stopping_cb])\n","```\n","\n","Another useful callback is [`ModelCheckpoint`](https://keras.io/api/callbacks/model_checkpoint/), which allows us to define an interval to automatically save a model or its weights. This is fundamental if we want save a trained model for later use, pick up training at a later date or ensure progress isn't lost in case of a disconnection when working in the cloud ([`BackUpAndRestore`](https://keras.io/api/callbacks/backup_and_restore/) callback can also be used for this purpose).\n","\n","```python\n","model.compile(loss=..., optimizer=...,\n","              metrics=['accuracy'])\n","\n","checkpoint_filepath = '/tmp/checkpoint'\n","model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    save_freq='epoch',\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","\n","# Model weights are saved at the end of every epoch, if it's the best seen so far.\n","model.fit(epochs=..., callbacks=[model_checkpoint_callback])\n","```\n","\n","For instance, the previous code example will save the model's weights at the end of each epoch only if the `val_accuracy` obtained was greater than the current best (overwriting the previous best weights). `ModelCheckpoint` can be configured to also save the model's definition (`save_weights_only=False`), save every N batches (`save_freq=N`) or save every weight configuration (`save_best_only=False`), among others.\n","\n","Saved weights can be later loaded using the `load_weights()` method. Notice that if we only saved the weights of the model, we'll need to rebuild the model first using the exact same architecture (if we saved the entire model, we can load it using `keras.models.load_model()` instead).\n","```python\n","# The model weights (that are considered the best) are loaded into the model.\n","model.load_weights(checkpoint_filepath)\n","```\n","\n","There are several different predefined callbacks we can use or even define our own. You can read more about callbacks in the documentation: https://keras.io/api/callbacks/"]},{"cell_type":"markdown","source":["## Saving and Loading ##\n","\n","You can also manually save a model at any time using the `save` method of the `Model` class or the `save_model` function from the `keras.saving` package.\n","\n","```\n","model = keras.models.Sequential(...)\n","model.save('model.keras')\n","// Alternatively\n","keras.saving.save_model(model, 'model.keras')\n","```\n","\n","Similarly, models can be loaded using the `load_model` function from the same `keras.saving` package.\n","\n","```\n","model = keras.saving.load_model('model.keras')\n","```\n","\n","You can read more about it in the [official documentation](https://keras.io/2.16/api/models/model_saving_apis/model_saving_and_loading/) and the [serialization and saving guide](https://keras.io/guides/serialization_and_saving/).\n","\n"],"metadata":{"id":"BJYPD4-rOyza"}},{"cell_type":"markdown","metadata":{"id":"UmAIpyBONULD"},"source":["## Practice\n","\n","For this practice exercise we'll build a 'simple' multilayer neural network classifier for the [FMNIST dataset](https://keras.io/api/datasets/fashion_mnist/). The FMNIST dataset is a collection of 70,000 grayscale images of clothing items of  $28\\times 28$  pixels each.\n"]},{"cell_type":"code","metadata":{"id":"4c9Nk5R8dKTr","executionInfo":{"status":"ok","timestamp":1727219731624,"user_tz":360,"elapsed":8882,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","%matplotlib inline\n","\n","keras.backend.clear_session()\n","tf.random.set_seed(42)\n","np.random.seed(42)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["The dataset is already divided into 60,000 images for training and 10,000 for testing. Don't forget to reserve 10% of your data as a validation set (you can set it up manually or through the `validation_split` parameter in `fit()`)."],"metadata":{"id":"CvpysVHaZsYS"}},{"cell_type":"code","source":["fmnist = keras.datasets.fashion_mnist\n","(X_train, y_train), (X_test, y_test) = fmnist.load_data()"],"metadata":{"id":"jm7VJk2VUSck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727219742108,"user_tz":360,"elapsed":1186,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}},"outputId":"cb802967-91cc-45f2-e76b-724d1e3d3084"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"udNK2A2ocrhn"},"source":["We can now check the properities of our loaded dataset by reviewing its shape. Keras will load the features dataset with every image represented as a $28 \\times 28$ array, and each pixel intensity represented by an integer between 0 and 255.\n","\n","The class label will simply correspond to the clothing category (0 to 9) according to the following key:\n","\n","0.\tT-shirt/top\n","1.\tTrouser\n","2.\tPullover\n","3.\tDress\n","4.\tCoat\n","5.\tSandal\n","6.\tShirt\n","7.\tSneaker\n","8.\tBag\n","9.\tAnkle boot"]},{"cell_type":"code","source":["classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt',\n","           'Sneaker','Bag','Ankle boot']"],"metadata":{"id":"bavt_LPPzC7Q","executionInfo":{"status":"ok","timestamp":1727219764346,"user_tz":360,"elapsed":935,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sNsXBahdKQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727219776943,"user_tz":360,"elapsed":220,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}},"outputId":"c0993104-4947-407a-9b3a-4d2660cbcef7"},"source":["print(X_train.shape)\n","print(y_train.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(60000,)\n"]}]},{"cell_type":"markdown","source":["Finally, we'll scale the range of the pixel values from $[0-255]$ into $[0-1]$."],"metadata":{"id":"qI5NmBlYakVA"}},{"cell_type":"code","source":["X_train = X_train / 255\n","X_test = X_test / 255"],"metadata":{"id":"j3alKFX1ZXYW","executionInfo":{"status":"ok","timestamp":1727219787674,"user_tz":360,"elapsed":264,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMCy7ampfIMp"},"source":["We can visualize a sample image from the dataset with its corresponding label."]},{"cell_type":"code","metadata":{"id":"dztDqJ7tfNFe","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1727219811381,"user_tz":360,"elapsed":633,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}},"outputId":"c513c165-3452-4065-dd43-eee72bd6c6b9"},"source":["# Example of a picture, change the index to visualize different pictures\n","index = 201\n","plt.imshow(X_train[index], cmap='Greys')\n","plt.axis('off')\n","print(\"y = \" + classes[y_train[index]])"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["y = Sandal\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Lastly, we'll define a function to easily visualize training results."],"metadata":{"id":"zZj-XZjOTFSC"}},{"cell_type":"code","source":["def plot_results(history):\n","  fig, (ax1, ax2) = plt.subplots(2)\n","  ax1.plot(history.history['accuracy'])\n","  ax1.plot(history.history['val_accuracy'])\n","  ax1.legend(['train_acc', 'val_acc'], loc='upper left')\n","\n","  ax2.plot(history.history['loss'])\n","  ax2.plot(history.history['val_loss'])\n","  ax2.legend(['train_loss', 'val_loss'], loc='upper left')"],"metadata":{"id":"Z5rd5GeUcuf_","executionInfo":{"status":"ok","timestamp":1727219828270,"user_tz":360,"elapsed":196,"user":{"displayName":"Julio Guillermo Arriaga Blumenkron","userId":"18360755152457408573"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drwwKrpxhfPF"},"source":["### Building the model\n","Build a Sequential fully connected NN to classify images into their corresponding category. Notice that, since the input images have a $(28, 28)$ shape, we'll need to first \"flatten\" them into a $(784, 1)$ feature vector. We can use a [`Flatten` layer](https://keras.io/api/layers/reshaping_layers/flatten/) at the start of our model to achieve this. Don't forget to add an [`Input` layer](https://keras.io/api/layers/core_layers/input/) as the first layer in your model to specify the input's shape.\n","\n","Next add some hidden dense layers. Use `He_normal` initialization and the `ELU` activation function. Make it a 'big' network with at least 10 hidden layers of 100 neurons each.\n","\n","When adding several similar layers, we can use the `add()` method of the `model` inside of a loop. For instance:\n","\n","```python\n","model = keras.models.Sequential()\n","for i in range(10):\n","  model.add(keras.layers.Dense())\n","```\n","\n","For the output layer, you'll need 10 units with a `softmax` activation, since the dataset has 10 different classes with no overlapping."]},{"cell_type":"code","metadata":{"id":"6l4TWtBvhh5E"},"source":["model1 ="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"arR85JindSJ5"},"source":["### Compiling and training the model\n","\n","Using `SGD` optimization with Nesterov momentum and [`sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) as your loss function, train the network on the FMNIST dataset for 50 epochs. Notice we need to use `sparse_categorical_crossentropy` instead of `categorical_crossentropy` since our targets are given as an integer between $[0-9]$."]},{"cell_type":"code","metadata":{"id":"6xx_1TMWdXxw"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot your results.\n"],"metadata":{"id":"Y3N3K7KMHdqj"}},{"cell_type":"code","source":[],"metadata":{"id":"KAWN2IEiH6mP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feel free to experiment with different learning rates and momentums, plot each configuration's results and compare them. If you notice the network might be underfitting, go back and increase its capacity."],"metadata":{"id":"7J0L1ghFXq0k"}},{"cell_type":"markdown","metadata":{"id":"r0nxMFEtiaDy"},"source":["Repeat the experiment using `Nadam` optimization and early stopping (with `patience=10` and restroring the best weights). Does it produce a better model?\n","Note that since we'll be using early stopping, you can crank up the total number of epochs, knowing that the model won't be overfitted."]},{"cell_type":"code","metadata":{"id":"Zk4i3cPCkgvl"},"source":["keras.backend.clear_session() # Resets all states created by keras, advisable when creating many models to free memory\n","# Create the model again so weights are initilized back to random\n","model2 = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKRInbzOdUII"},"source":["Now try adding `BatchNormalization` layers before each hidden layers' activation function (as well as after the `Flatten` layer) and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"]},{"cell_type":"code","metadata":{"id":"PcpmPajxdYZm"},"source":["keras.backend.clear_session()\n","# Create the model again so weights are initilized back to random\n","model3 = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_oEyA9XRdV0f"},"source":["Try regularizing the model with dropout by adding `Dropout` layers. See if you can achieve better accuracy (experiment with their number, placement and `rate`). Include a `ModelCheckpoint` callback to save your model's best weights."]},{"cell_type":"code","metadata":{"id":"YuZXqOP2dZNJ"},"source":["keras.backend.clear_session()\n","# Create the model again so weights are initilized back to random\n","model4 = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, test your chosen model using the test set. Did you get a good result?"],"metadata":{"id":"LAgSiwSG9POd"}},{"cell_type":"code","source":[],"metadata":{"id":"4tfbANUF9awe"},"execution_count":null,"outputs":[]}]}