{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Image Data Augmentation with Keras\n","\n","In this tutorial we'll take a look at one possible scenario when working with CNN: having to train an image-classification model using very little data. A \"few\" samples can mean anywhere from a few hundred to a few tens of thousands of images. As a practical example, we'll focus on classifying images as dogs or cats in a dataset containing 5,000 pictures of cats and dogs (2,500 cats, 2,500 dogs). We'll use 2000 pictures for training, 1000 for validation, and 2000 for testing."],"metadata":{"id":"GgLOWkz_1WH0"}},{"cell_type":"markdown","source":["## Preparation\n","\n","This section will setup our environment, mount GDrive, and and connect to Kaggle."],"metadata":{"id":"ZyZLWYtd3J9D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wJYdVsfYO4G"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Choose your directory where you would like to save the dataset\n","%cd \"/content/drive/MyDrive/...\""],"metadata":{"id":"NstS96PzvG6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAvULJSuiAOd"},"outputs":[],"source":["# Go to kaggle.com, account, get API key and upload it\n","from google.colab import files\n","files.upload()"]},{"cell_type":"code","source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"6aWWPAo8iMFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check kaggle is working\n","!kaggle datasets list"],"metadata":{"id":"Ke97I-81iWgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download dogs vs cats dataset\n","# Might need to accept competition terms first\n","# https://www.kaggle.com/competitions/dogs-vs-cats/data\n","!kaggle competitions download -c dogs-vs-cats"],"metadata":{"id":"xwGOGq6giYWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -qq dogs-vs-cats.zip"],"metadata":{"id":"eWA0FUvzix1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -qq train.zip"],"metadata":{"id":"TtcN1egijJIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have downloaded and unzziped the data, we will take a small sample and divide it into training/validation/testing sets using the following structure:\n","\n","```\n","cats_vs_dogs_small/\n","...train/\n","......cat/         \n","......dog/         \n","...validation/\n","......cat/         \n","......dog/         \n","...test/\n","......cat/         \n","......dog/\n","```\n","\n","Notice that we will only take a small sample of the original dataset to simulate training on a smaller dataset (and to accelerate training). We can expect many real world image datasets to follow a similar organization scheme, with separate folders for each class."],"metadata":{"id":"osMSbmAt3ioZ"}},{"cell_type":"code","source":["import os, shutil, pathlib\n","\n","original_dir = pathlib.Path(\"train\")\n","new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n","\n","def make_subset(subset_name, start_index, end_index):\n","    for category in (\"cat\", \"dog\"):\n","        dir = new_base_dir / subset_name / category\n","        os.makedirs(dir)\n","        fnames = [f\"{category}.{i}.jpg\"\n","                  for i in range(start_index, end_index)]\n","        for fname in fnames:\n","            shutil.copyfile(src=original_dir / fname,\n","                            dst=dir / fname)\n","\n","make_subset(\"train\", start_index=0, end_index=1000)\n","make_subset(\"validation\", start_index=1000, end_index=1500)\n","make_subset(\"test\", start_index=1500, end_index=2500)"],"metadata":{"id":"FWZe0QBNjIL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, shutil, pathlib\n","\n","original_dir = pathlib.Path(\"train\")\n","new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")"],"metadata":{"id":"OoQbh5wQlgI6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building the model\n","\n","We are now ready to build a CNN. We will opt for a \"classic\" architecture consisting of 3 x 3 convolution layers with ReLu activation functions, interspaced with 2 x 2 max-pooling layers.\n","\n","Output will consist of a single dense layer of one unit with a sigmoid activation function, since we only have two possible classes (cats vs dogs).\n","\n","Notice the first two layers of the model. Input allows us to specify sample images' dimensions (180 x 180, with 3 channels). We're also using a rescaling layer to change images' values to 0 - 255 range."],"metadata":{"id":"_jBschb-4Q5W"}},{"cell_type":"code","source":["from tensorflow import keras\n","from keras import layers\n","\n","model = keras.Sequential([keras.Input(shape=(180, 180, 3)),\n","                          layers.Rescaling(1./255),\n","                          layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n","                          layers.Flatten(),\n","                          layers.Dense(1, activation=\"sigmoid\")])\n","\n","model.summary()"],"metadata":{"id":"XUXIy5TikNFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"],"metadata":{"id":"EJMDnkU9lhgk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will now use the [`image_dataset_from_directory function`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) to easily load our sample images into a ```Dataset```. The function will automatically\n","* Read the picture files.\n","* Decode the JPEG content to RGB grids of pixels.\n","* Convert these into floating-point tensors.\n","* Assign a label based on parent folder.\n","* Resize them to a shared size (we'll use 180 Ã— 180).\n","* Pack them into batches (we'll use batches of 32 images).\n","\n","The use of a dataset generator is fundamental when working with huge amounts of data, since generally we won't be able to load all of them into memory. A data generator will instead only load the batches into memory on demand."],"metadata":{"id":"vxcayye86UMg"}},{"cell_type":"code","source":["from keras.utils import image_dataset_from_directory\n","\n","train_dataset = image_dataset_from_directory(\n","    new_base_dir / \"train\",\n","    image_size=(180, 180),\n","    batch_size=32)\n","\n","validation_dataset = image_dataset_from_directory(\n","    new_base_dir / \"validation\",\n","    image_size=(180, 180),\n","    batch_size=32)\n","\n","test_dataset = image_dataset_from_directory(\n","    new_base_dir / \"test\",\n","    image_size=(180, 180),\n","    batch_size=32)"],"metadata":{"id":"XE7OyPxzllSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for data_batch, labels_batch in train_dataset:\n","  print(\"data batch shape:\", data_batch.shape)\n","  print(\"labels batch shape:\", labels_batch.shape)\n","  break"],"metadata":{"id":"n-Ys8_psm5YI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are now ready to start training our model. We'll add a callback to save the best model's weights based on the validation loss value."],"metadata":{"id":"KjTKjQCf8BJA"}},{"cell_type":"code","source":["callbacks = [keras.callbacks.ModelCheckpoint(\n","    filepath=\"convnet_from_scratch.keras\", save_best_only=True, monitor=\"val_loss\")]\n","\n","history = model.fit(train_dataset, epochs=11,\n","                    validation_data=validation_dataset, callbacks=callbacks)"],"metadata":{"id":"Rb5C_rYPnCkW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.figure(figsize=(16, 4))\n","\n","plt.subplot(121)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.subplot(122)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"IEZBuo9Szxf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After only a few epochs we can readily observe the beggining of overfitting (notice the widening gap between training and validation). This is to be expected due to the (relatively) small amount of training images.\n","\n","If we test accuracy of the model against the testing set, the results (as expected) are not great."],"metadata":{"id":"3_fnUbpC8XQc"}},{"cell_type":"code","source":["test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"id":"IU41tQDEoXTD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can try fighting overfitting using some of the previously discussed  techniques (like dropout layers or l2 regularization), however another powerful tool at our disposal is data augmentation.\n","\n","Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that, at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data so it can generalize better."],"metadata":{"id":"4hAX_z0f9QhV"}},{"cell_type":"markdown","source":["We'll create a data augmentation model that will at random flip the images horizontally, rotate them at most $10\\% * 2\\pi$ and zoom at most $20\\%$. You can find a list of all available [image augmentation layers here](https://keras.io/api/layers/preprocessing_layers/image_augmentation/)."],"metadata":{"id":"vMx7Ze6b1VJs"}},{"cell_type":"code","source":["data_augmentation = keras.Sequential([layers.RandomFlip(\"horizontal\"),\n","                                      layers.RandomRotation(0.1),\n","                                      layers.RandomZoom(0.2)], name='data_augmentation')"],"metadata":{"id":"mgedHw2Bog5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 10))\n","for images, _ in train_dataset.take(1):\n","    for i in range(9):\n","        augmented_images = data_augmentation(images) # Keras functional API\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","        plt.axis(\"off\")"],"metadata":{"id":"HfDd-Yfsoo60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now add these augmentation layers to our previous model and try again. Overfitting should now be significantly lower, so we can train for an extended number of epochs."],"metadata":{"id":"4OcCqtU5_ZnA"}},{"cell_type":"code","source":["model = keras.Sequential([keras.Input(shape=(180, 180, 3)),\n","                          data_augmentation,\n","                          layers.Rescaling(1./255),\n","                          layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n","                          layers.MaxPooling2D(pool_size=2),\n","                          layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n","                          layers.Flatten(),\n","                          layers.Dense(1, activation=\"sigmoid\")])\n","\n","model.summary()"],"metadata":{"id":"UxnMOyG35PX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"],"metadata":{"id":"DyepxYWGpupN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Notice** our ```ModelCheckpoint``` callback will, once again, save the best model's weights. This will allow us to rollback to a previous version if we notice overfitting or pick up where we left off in a future session."],"metadata":{"id":"IgjizyNT_ryN"}},{"cell_type":"code","source":["callbacks = [keras.callbacks.ModelCheckpoint(\n","    filepath=\"convnet_from_scratch_with_augmentation.keras\",\n","    save_best_only=True, monitor=\"val_loss\")]\n","\n","history = model.fit(train_dataset, epochs=30,\n","                    validation_data=validation_dataset, callbacks=callbacks)"],"metadata":{"id":"sjrDtmu6pyea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(16, 4))\n","\n","plt.subplot(121)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.subplot(122)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"TlvxmAb_4_so"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training and validation scores should now remain closer, meaning we have effectively fought off overfitting. Similarly, test score should now be higher than before."],"metadata":{"id":"GDDiPfI_5pfx"}},{"cell_type":"code","source":["test_model = keras.models.load_model(\n","    \"convnet_from_scratch_with_augmentation.keras\")\n","test_loss, test_acc = test_model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_acc:.3f}\")"],"metadata":{"id":"R4cl8chip22c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A Note on Data Augmentation with ImageDataGenerator\n","In the past, Keras `ImageDataGenerator` was the suggested method for generating batches of tensor image data with real-time data augmentation. However, as of TF 2.9.0 `ImageDataGenerator` has been marked as deprecated, so using it in new code is not advisable."],"metadata":{"id":"M_zahfEG6Rwz"}},{"cell_type":"code","source":["datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.2)\n","\n","train_generator = datagen.flow_from_directory(new_base_dir / \"train\",\n","                            target_size=(180, 180), batch_size=32)\n","\n","for X_batch, y_batch in train_generator:\n","    for i in range(0, 6):\n","        plt.subplot(2,3,i+1)\n","        plt.imshow(X_batch[i]/255)\n","        plt.axis('off')\n","    break"],"metadata":{"id":"BbBxz6027cx7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbecFxDVVvqV"},"source":["*Parts of this tutorial have been adapted from [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) By AurÃ©lien GÃ©ron*"]}]}